{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using ```pytorch``` - Logistic Regression Classifier\n",
    "The first thing we're going to do, as usual, is begin by importing libraries and modules we're going to use today. We're introducing a new library, called ```datasets```, which is part of the ```huggingface``` unviverse. \n",
    "\n",
    "```datasets``` provides easy access to a wide range of example datasets which are widely-known in the NLP world, it's worth spending some time looking around to see what you can find. For example, here are a collection of [multilabel classification datasets](https://huggingface.co/datasets?task_ids=task_ids:multi-class-classification&sort=downloads).\n",
    "\n",
    "We'll be working with the ```huggingface``` ecosystem more and more as we progress this semester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system tools\n",
    "import os\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# huggingface datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# scikit learn tools\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# plotting tools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Creating a model class__\n",
    "\n",
    "The most effective way to work with ```pytorch``` is by using its built-in abstractions to create classes which describe specfic models.\n",
    "\n",
    "In the following cell, we're defining the Logistic Regression classifier model. Notice how we use the ```super()```, which means that our model _inherits_ everything from the parent class, such as backpropagation methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_input_features=10):            # default input features, can be overridden\n",
    "        super().__init__()                              # inherit from parent class\n",
    "        self.linear = nn.Linear(n_input_features, 1)    # one linear layer with single output\n",
    "\n",
    "    def forward(self, x):                               # how should one forward pass look?\n",
    "        x = self.linear(x)                              # pass the data through the linear layer\n",
    "        y_pred = torch.sigmoid(x)                       # squash the outputs through sigmoid layer\n",
    "        return y_pred                                   # return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "We're going to be working with actual text data data, specifically a subset of the well-known [GLUE Benchmarks](https://gluebenchmark.com/). These benchmarks are regularly used to test how well certain models perform across a range of different language tasks. We'll work today specifically with the Stanford Sentiment Treebank 2 (SST2) - you can learn more [here](https://huggingface.co/datasets/glue) and [here](https://nlp.stanford.edu/sentiment/index.html).\n",
    "\n",
    "The dataset we get back is a complex, hierarchical object with lots of different features. I recommend that you dig around a little and see what it contains. For today, we're going to work with only the training dataset right now, and we're going to split it into sentences and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sst2 dataset\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "# select the train split\n",
    "data = dataset[\"train\"]\n",
    "X = data[\"sentence\"]\n",
    "y = data[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Create document representations\n",
    "We're going to work with a bag-of-words model, which we can create quite simply using the ```CountVectorizer()``` class available via ```scikit-learn```. You can read more about the defaul parameters of the vectorizer [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html).\n",
    "\n",
    "After we initialize the vectorizer, we first _fit_ this vectorizer to our data and then _transform_ the original data into the BoW representation.\n",
    "\n",
    "Possible follow up questions:\n",
    "\n",
    "- How does changing the parameters of ```CountVectorizer()``` affect model performance?\n",
    "- How would you implement your own ```CountVectorizer()```? What steps would be required and in what order?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "# vectorized training data\n",
    "X_vect = vectorizer.fit_transform(X)\n",
    "# to tensors\n",
    "X_vect = torch.tensor(X_vect.toarray(), dtype=torch.float)\n",
    "y = torch.tensor(list(y), dtype=torch.float)\n",
    "y = y.view(y.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Initialize parameters\n",
    "We then want to initialize parameters for our model to use. Like last week, use ```X_vect.shape``` to get the number of features for each document.\n",
    "\n",
    "We have binary classification problem, so the loss function we will use is the _Binary Cross Entropy_ function, seen here as ```BCELoss()```.\n",
    "\n",
    "Notice that we're using a new optimization algorithm called ```ADAM```. ADAM is a gradient descent algorithm which works in a way that is a bit more clever than regular Stochastic Gradient Descent. You can read more [here](http://optimization.cbe.cornell.edu/index.php?title=Adam) and the research paper describing ```ADAM``` can be found [here](https://arxiv.org/abs/1412.6980)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model\n",
    "n_samples, n_features = X_vect.shape\n",
    "model = Model(n_input_features=n_features)\n",
    "\n",
    "# define loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "Once all of the parameters are defined, we then train the model. Notice how the code here is basically identical to the code we saw last week when training a model for linear regression:\n",
    "\n",
    "- Define number of epochs to train for (iterations)\n",
    "- Get predictions from the model (```y_hat```)\n",
    "- Calculate loss based on ```y_hat``` and ```y```\n",
    "- Backpropagate the loss\n",
    "- Take step using gradient descent\n",
    "- Repeat up to n epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "epochs = 100\n",
    "print(\"[INFO:] Training classifier...\")\n",
    "loss_history = []\n",
    "for epoch in range(epochs):\n",
    "    # forward\n",
    "    y_hat = model(X_vect)\n",
    "\n",
    "    # backward\n",
    "    loss = criterion(y_hat, y)\n",
    "    loss_history.append(loss)\n",
    "\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "    # take step, reset\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # some print to see that it is running\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"epoch: {epoch+1}, loss = {loss.item():.4f}\")\n",
    "\n",
    "print(\"[INFO:] Finished traning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "Now that we've trained a model for 1000 steps, we want to know how well it actually performs when classifying the data. We can test this by just checking how accurately it classifies the training data.\n",
    "\n",
    "The first thing we do is get predictions from the model for every data point, which we then convert to a ```numpy``` array. Using a ```numpy``` array allows us to easily use the ```classification_report``` available from ```scikit-learn```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "predicted = model(X_vect).detach().numpy()\n",
    "print(classification_report(y, \n",
    "                            np.where(predicted > 0.5, 1, 0),\n",
    "                            target_names = [\"Negative\", \"Positive\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the context of machine learning, the name ```recall``` is used in terms of ```sensitivity```.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ConfusionMatrix](../img/confusionMatrix.jpg \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot loss history\n",
    "\n",
    "When we plot the history of the loss function, we're able to see _how well_ our model is learning over time. I recommend that you read through [this document](https://www.baeldung.com/cs/learning-curve-ml) to get a feel for what we can learn from these kinds of plots.\n",
    "\n",
    "We'll be coming back to this again in later weeks. For now, the point is that the curve of our loss function should be smooth and decreasing regularly over time if the model is learning well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_H = [val.item() for val in loss_history]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(loss_H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "- Do you think the model is learning well?\n",
    "- What problems do we have with this approach so far?\n",
    "    - Hint: we're only using the _training_ data. What about [_test data_](https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets)?\n",
    "- Where can the model be improved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
