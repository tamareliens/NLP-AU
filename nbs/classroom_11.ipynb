{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classroom 11 - Text Generation with HuggingFace transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this class, we'll look at how we can easily use HuggingFace Transformers for both text generation and for text-to-text models like (m)T5.\n",
    "\n",
    "As with many tasks, Huggingface makes the basic aspects of text generation extremely simple to implement.\n",
    "\n",
    "You can read about HuggingFace's text generation pipelines [here](https://huggingface.co/tasks/text-generation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text completion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the ```pipeline``` class, intialize it with some parameters, and then we get generating!\n",
    "\n",
    "You can find a full range of models which are available for text generation via the Huggingface model zoo [here](https://huggingface.co/models?pipeline_tag=text-generation).\n",
    "\n",
    "Try experimenting with the following model architectures:\n",
    "\n",
    "- GPT style\n",
    "    - GPT-2, GPT-J-6B\n",
    "- Meta AI's OPT \n",
    "    - OPT-350m, OPT-2.7B, etc\n",
    "- BigScience BLOOM\n",
    "    - BLOOM-560m, BLOOM 7B1, etc\n",
    "\n",
    "__Questions__\n",
    "- Do some perform better than others?\n",
    "- How much of an impact does scale make on compute?\n",
    "- Is the trade off worth in, in terms of compute time compared to quality of output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model = 'gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give the model a text prompt, define the max length of tokens, and how many examples we want to be generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = generator(\"Hello, I'm a language model\", \n",
    "                    max_length = 50, \n",
    "                    num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs are then returned as a list of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can index specific examples like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0].get(\"generated_text\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text summarization\n",
    "\n",
    "Summarization is the prototype text-to-text task, where we are taking an input sequence and trying to train model to accurately produce a summary of the contents. \n",
    "\n",
    "Again, with Huggingface, it's easy to experiment with exisitng text summarization models. Check out their overview [here](https://huggingface.co/tasks/summarization).\n",
    "\n",
    "Once more, check out the model zoo for existing models which have been explicitly finetuned for text summarization tasks [here](https://huggingface.co/models?filter=summarization).\n",
    "\n",
    "\n",
    "\n",
    "__Questions__\n",
    "- What do you think of the summaries?\n",
    "- How do finetuned models compare to non-finetuned models like T5?\n",
    "- What do you think of the multilingual models like mt5?\n",
    "- If you are a Danish speaker, try the danT5 model. How well does it perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model = \"t5-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define a text that we want to summarise. I've chosen the first part of a recent news article in The Guardian - feel free to change this to whatever you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" More than 200 Indonesian fruit pickers have sought diplomatic help since July after facing difficulties working in Britain this season, the nation’s embassy has revealed.\n",
    "\n",
    "The Guardian has spoken to a pair of workers sent to a farm in Scotland that supplies berries to M&S, Waitrose, Tesco and Lidl. They claim pickers were sent back to the caravan if they could not work fast enough and left with large debts to repay.\n",
    "\n",
    "The embassy says the true number of people experiencing problems is likely to be far higher, as many were seeking help on behalf of several workers at the same farms – and others would not have the confidence to approach the embassy.\n",
    "Agung was expecting six months of well-paid farming work in Britain.\n",
    "\n",
    "It says the most common reported problem is a lack of work at farms, especially for those who arrived very late in the season. Some did not start until the harvest was all but over, giving them little opportunity to repay debts incurred when they signed up.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, generate out summaries using the ```summarizer``` we defined above, which returns a list of dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = summarizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can get just the summary text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary[0].get(\"summary_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__\n",
    "\n",
    "Here we are only using pretrained models to generate or summarize text. We haven't looked at, for example, how we might train or finetune models on specific tasks. That would take a bit more time than we have available in this class.\n",
    "\n",
    "If you want to dig into that in a bit more detail, HuggingFace offer many high-quality walkthroughs via [their public Github repo](https://github.com/huggingface/transformers).\n",
    "\n",
    "In particular, check out the directory called [Notebooks](https://github.com/huggingface/transformers/tree/main/notebooks) and also the one called [Examples](https://github.com/huggingface/transformers/tree/main/examples). The former are more pedagogical and explain things step-by-step; the latter are more advanced examples of how to fine-tune models effectively using the kinds of Python scripting skills you've developed in this course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
