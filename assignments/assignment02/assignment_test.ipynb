{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible solution to assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a worked solution to assignment 3, training a neural network classifier with ```pytorch```.\n",
    "\n",
    "Please note that this is *not* comprehensive - this is not then only solution, with all others being wrong. This is just a possible solution, based on me having spent some time looking at the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading packages\n",
    "\n",
    "Note how here I'm using ```TfidfVectorizer()``` instead of ```CountVectorizer()```. This seemed to perform better empirically, producing smoother learning curves and higher accuracies on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T11:45:49.822394Z",
     "iopub.status.busy": "2022-10-13T11:45:49.821695Z",
     "iopub.status.idle": "2022-10-13T11:45:51.478553Z",
     "shell.execute_reply": "2022-10-13T11:45:51.477804Z",
     "shell.execute_reply.started": "2022-10-13T11:45:49.822335Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# system tools\n",
    "import os\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# huggingface datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# scikit learn tools\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# plotting tools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Neural Network model class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how here I'm using a _ReLU_ activation function. After experimenting a little, I found that _ReLU_ was overfitting much less quickly than using _Sigmoid_ activation functions. The final layer is still _Sigmoid_ as we're trying to make a binary prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T11:45:52.446986Z",
     "iopub.status.busy": "2022-10-13T11:45:52.446375Z",
     "iopub.status.idle": "2022-10-13T11:45:52.458121Z",
     "shell.execute_reply": "2022-10-13T11:45:52.457184Z",
     "shell.execute_reply.started": "2022-10-13T11:45:52.446932Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(n_input_features, 100)\n",
    "        self.linear2 = nn.Linear(100, 10)\n",
    "        self.linear3 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        leaky_relu = nn.LeakyReLU(0.2)\n",
    "        # Linear -> ReLU\n",
    "        x = self.linear1(x)\n",
    "        x = leaky_relu(x)\n",
    "        # Linear -> ReLU\n",
    "        x = self.linear2(x)\n",
    "        x = leaky_relu(x)\n",
    "        # Linear -> Sigmoid\n",
    "        x = self.linear3(x)\n",
    "        y_pred = torch.sigmoid(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load data__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, I decided to use the \"Rotten Tomatoes\" dataset from Huggingface. Technically, any other ```Huggingface``` dataset should fit in here, as long as it has training-validation-test datasets and binary labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T11:45:53.916946Z",
     "iopub.status.busy": "2022-10-13T11:45:53.916187Z",
     "iopub.status.idle": "2022-10-13T11:45:58.969976Z",
     "shell.execute_reply": "2022-10-13T11:45:58.968775Z",
     "shell.execute_reply.started": "2022-10-13T11:45:53.916892Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the sst2 dataset\n",
    "dataset = load_dataset(\"rotten_tomatoes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start by explicitly separating the different splits in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T11:45:58.981711Z",
     "iopub.status.busy": "2022-10-13T11:45:58.981358Z",
     "iopub.status.idle": "2022-10-13T11:45:58.987989Z",
     "shell.execute_reply": "2022-10-13T11:45:58.986625Z",
     "shell.execute_reply.started": "2022-10-13T11:45:58.981679Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select the train split\n",
    "train = dataset[\"train\"]\n",
    "val = dataset[\"validation\"]\n",
    "test = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Vectorize__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then define how to use my vectorizer class. I originally trained by using ```lowercase=True```and using removing highly common English words (what are called \"stopwords\" in text mining literature). However, this was a bad idea - the data is already lowercase, and removing stopwords reduced performance. I suspect that this is because I was actually removing a lot of common words which are shared across training and test data, meaning it was harder for the model to generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T11:51:48.202285Z",
     "iopub.status.busy": "2022-10-13T11:51:48.201605Z",
     "iopub.status.idle": "2022-10-13T11:51:48.227966Z",
     "shell.execute_reply": "2022-10-13T11:51:48.227198Z",
     "shell.execute_reply.started": "2022-10-13T11:51:48.202228Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2),\n",
    "                            #lowercase=True, \n",
    "                            #stop_words=\"english\", \n",
    "                            max_df=0.9, \n",
    "                            min_df=0.1,\n",
    "                            max_features=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first vectorize the training data and conver the labels to something that ```pytorch``` can use. \n",
    "\n",
    "After than, I transform the validation and test data using the vectorizer that was fit to the training data. Be careful here that you don't use ```.fit_transform()``` on the validation and training data again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T11:51:49.033759Z",
     "iopub.status.busy": "2022-10-13T11:51:49.033563Z",
     "iopub.status.idle": "2022-10-13T11:51:49.428600Z",
     "shell.execute_reply": "2022-10-13T11:51:49.427609Z",
     "shell.execute_reply.started": "2022-10-13T11:51:49.033741Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vectorized training data -> to tensors\n",
    "train_vect = vectorizer.fit_transform(train[\"text\"])\n",
    "train_vect = torch.tensor(train_vect.toarray(), dtype=torch.float)\n",
    "\n",
    "# labels\n",
    "train_label = torch.tensor(list(train[\"label\"]), dtype=torch.float)\n",
    "train_label = train_label.view(train_label.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T11:51:49.430007Z",
     "iopub.status.busy": "2022-10-13T11:51:49.429726Z",
     "iopub.status.idle": "2022-10-13T11:51:49.463608Z",
     "shell.execute_reply": "2022-10-13T11:51:49.463193Z",
     "shell.execute_reply.started": "2022-10-13T11:51:49.429982Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vectorized validation data -> to tensors\n",
    "val_vect = vectorizer.transform(val[\"text\"])\n",
    "val_vect = torch.tensor(val_vect.toarray(), dtype=torch.float)\n",
    "\n",
    "# labels\n",
    "val_label = torch.tensor(list(val[\"label\"]), dtype=torch.float)\n",
    "val_label = val_label.view(val_label.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T11:51:49.465135Z",
     "iopub.status.busy": "2022-10-13T11:51:49.464870Z",
     "iopub.status.idle": "2022-10-13T11:51:49.503738Z",
     "shell.execute_reply": "2022-10-13T11:51:49.503085Z",
     "shell.execute_reply.started": "2022-10-13T11:51:49.465117Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vectorized test data -> to tensors\n",
    "test_vect = vectorizer.transform(test[\"text\"])\n",
    "test_vect = torch.tensor(test_vect.toarray(), dtype=torch.float)\n",
    "\n",
    "# labels\n",
    "test_label = torch.tensor(list(test[\"label\"]), dtype=torch.float)\n",
    "test_label = test_label.view(test_label.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Parameters__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then initialise my model just like we did in class last time. The only difference here is that I'm adding a fairly small learning rate to ```ADAM``` to avoid overfitting - note that ```1e-4``` is scientific notation, so that's 0.0001.\n",
    "\n",
    "This learning rate was decided upon through trial and error. We'll see in class soon how we can do _hyperparameter search_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T11:52:39.193808Z",
     "iopub.status.busy": "2022-10-13T11:52:39.192969Z",
     "iopub.status.idle": "2022-10-13T11:52:39.203543Z",
     "shell.execute_reply": "2022-10-13T11:52:39.202474Z",
     "shell.execute_reply.started": "2022-10-13T11:52:39.193748Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize model\n",
    "n_samples, n_features = train_vect.shape\n",
    "model = Model(n_input_features=n_features)\n",
    "\n",
    "# define loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                              lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Train__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I create empty lists into which I'm going to save the training and validation loss after every epoch. This is because we can actually learn a lot about _how well_ a model is learning by inspecting the loss curves. There's a nice introduction to this idea [here](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/).\n",
    "\n",
    "Note that we could do the same for an _accuracy curve_ - i.e. how much is accuracy improving over time. \n",
    "\n",
    "- How might we implement that?\n",
    "- What would we expect to see if a model is learning well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T11:52:39.962303Z",
     "iopub.status.busy": "2022-10-13T11:52:39.962132Z",
     "iopub.status.idle": "2022-10-13T11:52:39.974454Z",
     "shell.execute_reply": "2022-10-13T11:52:39.973830Z",
     "shell.execute_reply.started": "2022-10-13T11:52:39.962288Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for plotting\n",
    "train_loss_history = []\n",
    "val_loss_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train for 1000 epochs, and the loop here is very similar to what we've worked on in-class. The only difference is that after we've run the backprop and updated the gradients, we then test how well the model performs on the validation data during this epoch.\n",
    "\n",
    "Notice that we need to do two things here - firstly, we set ```torch.no_grad()``` to make sure that we don't accidentally update any gradients that we don't want to update. We also then have to set the model to evaluation mode using ```model.eval()```. This make sure that we don't do a forward pass over the validation data and end up learning from it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T11:52:39.975378Z",
     "iopub.status.busy": "2022-10-13T11:52:39.975209Z",
     "iopub.status.idle": "2022-10-13T11:52:47.377280Z",
     "shell.execute_reply": "2022-10-13T11:52:47.376608Z",
     "shell.execute_reply.started": "2022-10-13T11:52:39.975363Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train\n",
    "epochs = 1000\n",
    "print(\"[INFO:] Training classifier...\")\n",
    "\n",
    "# loop for epochs\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # forward\n",
    "    y_hat = model(train_vect)\n",
    "\n",
    "    # backward\n",
    "    loss = criterion(y_hat, train_label)\n",
    "    train_loss_history.append(loss)\n",
    "\n",
    "    # backpropagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # take step, reset\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Validation Loop \n",
    "    with torch.no_grad(): \n",
    "        # set to eval mode\n",
    "        model.eval() \n",
    "\n",
    "        # make predictions\n",
    "        predicted_outputs = model(val_vect) \n",
    "\n",
    "        # metrics\n",
    "        val_loss = criterion(predicted_outputs, val_label) \n",
    "\n",
    "        # append\n",
    "        val_loss_history.append(val_loss) \n",
    "\n",
    "    # some print to see that it is running\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"epoch: {epoch+1}, loss = {loss.item():.4f}\")\n",
    "\n",
    "print(\"[INFO:] Finished traning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Evaluate__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has trained, we can then inspect performance. Since we only trained on the train and validation splits, we can then check how our accuracy looks when evaluating the test data. This test data has not been at all during training, so it's completely new to the model. If it performs well on this held out data, that implies that the model has learned to generalize well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T11:52:48.867733Z",
     "iopub.status.busy": "2022-10-13T11:52:48.867555Z",
     "iopub.status.idle": "2022-10-13T11:52:48.881390Z",
     "shell.execute_reply": "2022-10-13T11:52:48.880659Z",
     "shell.execute_reply.started": "2022-10-13T11:52:48.867716Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "predicted = model(test_vect).detach().numpy()\n",
    "print(classification_report(test_label, \n",
    "                            np.where(predicted > 0.5, 1, 0),\n",
    "                            target_names = [\"Negative\", \"Positive\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also produce a simple plot showing how training and validation loss decrease over time. The blue line is the training loss; orange is the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-13T11:52:48.882400Z",
     "iopub.status.busy": "2022-10-13T11:52:48.882196Z",
     "iopub.status.idle": "2022-10-13T11:52:49.009984Z",
     "shell.execute_reply": "2022-10-13T11:52:49.009278Z",
     "shell.execute_reply.started": "2022-10-13T11:52:48.882380Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loss = [val.item() for val in train_loss_history]\n",
    "val_loss = [val.item() for val in val_loss_history]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(train_loss)\n",
    "ax.plot(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Given the loss curves and the classification report, how well is this model performing?\n",
    "- If we wanted to make improvements, what might we want to change?\n",
    "- How do we make principled decisions about which hyperparameters to change?\n",
    "  - Hint: One way to do it to just use [brute force](https://towardsdatascience.com/grid-search-for-model-tuning-3319b259367e)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
