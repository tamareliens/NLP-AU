# Class 5 - Exploring word embeddings with ```gensim```

Today we'll be looking at a bit more critically and analytically at how word embeddings encode semantic information. We'll do this by working in groups through a series of exercises that I've created. 

At the moment, we're not so focused on how we can train our own word embeddings. You can see an example of how we might do that in ```pytorch``` [here](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html), or more simply using ```gensim``` which we'll learn about today ([see here](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html#training-your-own-model)).

---
## Outline

- Brief chat about previous assignment
- Introducing the tasks
- Working in groups experimenting
- Bonus tasks if time allows it